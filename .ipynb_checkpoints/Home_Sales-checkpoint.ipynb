{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_KW73O2e3dw",
    "outputId": "fa5fd2b3-e2de-491b-ee1c-405317ba7ebc"
   },
   "outputs": [],
   "source": [
    "# Import findspark and initialize. \n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2XbWNf1Te5fM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/12 02:25:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Home Sales Analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOJqxG_RPSwp",
    "outputId": "7857ef9f-5b04-405d-f5aa-e535dfe7870c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "file_path = \"/Users/latifahjones/Desktop/Home_Sales/Resources/home_sales_revised.csv\"\n",
    "print(os.path.exists(file_path))  # This should return True if the file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "RoljcJ7WPpnm",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|                  id|    date|date_built| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|\n",
      "+--------------------+--------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|f8a53099-ba1c-47d...|  4/8/22|      2016|936923|       4|        3|       3167|   11733|     2|         1|  76|\n",
      "|7530a2d8-1ae3-451...| 6/13/21|      2013|379628|       2|        2|       2235|   14384|     1|         0|  23|\n",
      "|43de979c-0bf0-4c9...| 4/12/19|      2014|417866|       2|        2|       2127|   10575|     2|         0|   0|\n",
      "|b672c137-b88c-48b...|10/16/19|      2016|239895|       2|        2|       1631|   11149|     2|         0|   0|\n",
      "|e0726d4d-d595-407...|  1/8/22|      2017|424418|       3|        2|       2249|   13878|     2|         0|   4|\n",
      "|5aa00529-0533-46b...| 1/30/19|      2017|218712|       2|        3|       1965|   14375|     2|         0|   7|\n",
      "|131492a1-72e2-4a8...|  2/8/20|      2017|419199|       2|        3|       2062|    8876|     2|         0|   6|\n",
      "|8d54a71b-c520-44e...| 7/21/19|      2010|323956|       2|        3|       1506|   11816|     1|         0|  25|\n",
      "|e81aacfe-17fe-46b...| 6/16/20|      2016|181925|       3|        3|       2137|   11709|     2|         0|  22|\n",
      "|2ed8d509-7372-46d...|  8/6/21|      2015|258710|       3|        3|       1918|    9666|     1|         0|  25|\n",
      "|f876d86f-3c9f-42b...| 2/27/19|      2011|167864|       3|        3|       2471|   13924|     2|         0|  15|\n",
      "|0a2bd445-8508-4d8...|12/30/21|      2014|337527|       2|        3|       1926|   12556|     1|         0|  23|\n",
      "|941bad30-eb49-4a7...|  5/9/20|      2015|229896|       3|        3|       2197|    8641|     1|         0|   3|\n",
      "|dd61eb34-6589-4c0...| 7/25/21|      2016|210247|       3|        2|       1672|   11986|     2|         0|  28|\n",
      "|f1e4cef7-d151-439...|  2/1/19|      2011|398667|       2|        3|       2331|   11356|     1|         0|   7|\n",
      "|ea620c7b-c2f7-4c6...| 5/31/21|      2011|437958|       3|        3|       2356|   11052|     1|         0|  26|\n",
      "|f233cb41-6f33-4b0...| 7/18/21|      2016|437375|       4|        3|       1704|   11721|     2|         0|  34|\n",
      "|c797ca12-52cd-4b1...|  6/8/19|      2015|288650|       2|        3|       2100|   10419|     2|         0|   7|\n",
      "|0cfe57f3-28c2-472...| 10/4/19|      2015|308313|       3|        3|       1960|    9453|     2|         0|   2|\n",
      "|4566cd2a-ac6e-435...| 7/15/19|      2016|177541|       3|        3|       2130|   10517|     2|         0|  25|\n",
      "+--------------------+--------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "        \n",
    "        # Create a temporary view of the DataFrame\n",
    "        df.createOrReplaceTempView(\"home_sales\")\n",
    "        \n",
    "        # Display the DataFrame to confirm it loaded correctly\n",
    "        df.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the CSV file: {e}\")\n",
    "else:\n",
    "    print(f\"The file does not exist at the specified path: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6fkwOeOmqvq",
    "outputId": "bdded620-79c4-488d-c7a5-91c6799c419e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|year|average_price|\n",
      "+----+-------------+\n",
      "+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. What is the average price for a four bedroom house sold per year, rounded to two decimal places?\n",
    "average_price_query = \"\"\"\n",
    "SELECT\n",
    "    YEAR(date) AS year,\n",
    "    ROUND(AVG(price), 2) AS average_price\n",
    "FROM home_sales\n",
    "WHERE bedrooms = 4\n",
    "GROUP BY YEAR(date)\n",
    "ORDER BY year\n",
    "\"\"\"\n",
    "\n",
    "average_price_df = spark.sql(average_price_query)\n",
    "average_price_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l8p_tUS8h8it",
    "outputId": "65806e5f-6262-41c0-ff65-5107464e5c4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|year_built|average_price|\n",
      "+----------+-------------+\n",
      "|      2010|    292859.62|\n",
      "|      2011|    291117.47|\n",
      "|      2012|    293683.19|\n",
      "|      2013|    295962.27|\n",
      "|      2014|    290852.27|\n",
      "|      2015|     288770.3|\n",
      "|      2016|    290555.07|\n",
      "|      2017|    292676.79|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. What is the average price of a home for each year the home was built,\n",
    "# that have 3 bedrooms and 3 bathrooms, rounded to two decimal places?\n",
    "average_price_built_query = \"\"\"\n",
    "SELECT \n",
    "    date_built AS year_built, \n",
    "    ROUND(AVG(price), 2) AS average_price \n",
    "FROM \n",
    "    home_sales \n",
    "WHERE \n",
    "    bedrooms = 3 AND bathrooms = 3 \n",
    "GROUP BY \n",
    "    date_built \n",
    "ORDER BY \n",
    "    year_built\n",
    "\"\"\"\n",
    "\n",
    "# Run the query and create a DataFrame\n",
    "average_price_built_df = spark.sql(average_price_built_query)\n",
    "\n",
    "# Show the results\n",
    "average_price_built_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-Eytz64liDU",
    "outputId": "17119810-56ad-40c3-de5e-c3db57e43bcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|year_built|average_price|\n",
      "+----------+-------------+\n",
      "|      2010|    285010.22|\n",
      "|      2011|    276553.81|\n",
      "|      2012|    307539.97|\n",
      "|      2013|    303676.79|\n",
      "|      2014|    298264.72|\n",
      "|      2015|    297609.97|\n",
      "|      2016|     293965.1|\n",
      "|      2017|    280317.58|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. What is the average price of a home for each year the home was built,\n",
    "# that have 3 bedrooms, 3 bathrooms, with two floors,\n",
    "# and are greater than or equal to 2,000 square feet, rounded to two decimal places?\n",
    "average_price_built_query = \"\"\"\n",
    "SELECT \n",
    "    date_built AS year_built, \n",
    "    ROUND(AVG(price), 2) AS average_price \n",
    "FROM \n",
    "    home_sales \n",
    "WHERE \n",
    "    bedrooms = 3 \n",
    "    AND bathrooms = 3 \n",
    "    AND floors = 2 \n",
    "    AND sqft_living >= 2000 \n",
    "GROUP BY \n",
    "    date_built \n",
    "ORDER BY \n",
    "    year_built\n",
    "\"\"\"\n",
    "\n",
    "# Run the query and create a DataFrame\n",
    "average_price_built_df = spark.sql(average_price_built_query)\n",
    "\n",
    "# Show the results\n",
    "average_price_built_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUrfgOX1pCRd",
    "outputId": "17c25774-855e-4290-a4bd-a04902bdc13a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|view|average_price|\n",
      "+----+-------------+\n",
      "| 100|    1026669.5|\n",
      "|  99|   1061201.42|\n",
      "|  98|   1053739.33|\n",
      "|  97|   1129040.15|\n",
      "|  96|   1017815.92|\n",
      "|  95|    1054325.6|\n",
      "|  94|    1033536.2|\n",
      "|  93|   1026006.06|\n",
      "|  92|    970402.55|\n",
      "|  91|   1137372.73|\n",
      "|  90|   1062654.16|\n",
      "|  89|   1107839.15|\n",
      "|  88|   1031719.35|\n",
      "|  87|    1072285.2|\n",
      "|  86|   1070444.25|\n",
      "|  85|   1056336.74|\n",
      "|  84|   1117233.13|\n",
      "|  83|   1033965.93|\n",
      "|  82|    1063498.0|\n",
      "|  81|   1053472.79|\n",
      "+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "--- 0.9010839462280273 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# 6. What is the average price of a home per \"view\" rating, rounded to two decimal places,\n",
    "# having an average home price greater than or equal to $350,000? Order by descending view rating. \n",
    "# Although this is a small dataset, determine the run time for this query.\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# SQL query to calculate the average price of homes per view rating\n",
    "average_price_view_query = \"\"\"\n",
    "SELECT \n",
    "    view, \n",
    "    ROUND(AVG(price), 2) AS average_price \n",
    "FROM \n",
    "    home_sales \n",
    "WHERE \n",
    "    price >= 350000 \n",
    "GROUP BY \n",
    "    view \n",
    "ORDER BY \n",
    "    view DESC\n",
    "\"\"\"\n",
    "\n",
    "# Run the query and create a DataFrame\n",
    "average_price_view_df = spark.sql(average_price_view_query)\n",
    "\n",
    "# Show the results\n",
    "average_price_view_df.show()\n",
    "\n",
    "# Calculate and print the runtime\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAhk3ZD2tFy8",
    "outputId": "0a8f132d-40a8-4bd4-b5f2-2847e98427f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Cache the the temporary table home_sales.\n",
    "spark.sql(\"CACHE TABLE home_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4opVhbvxtL-i",
    "outputId": "38ec8487-795f-4550-b50c-fcc6f2b7c769"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. Check if the table is cached.\n",
    "spark.catalog.isCached('home_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5GnL46lwTSEk",
    "outputId": "09a16c73-194d-4371-95d1-ee64fe83b91c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|view|average_price|\n",
      "+----+-------------+\n",
      "| 100|    1026669.5|\n",
      "|  99|   1061201.42|\n",
      "|  98|   1053739.33|\n",
      "|  97|   1129040.15|\n",
      "|  96|   1017815.92|\n",
      "|  95|    1054325.6|\n",
      "|  94|    1033536.2|\n",
      "|  93|   1026006.06|\n",
      "|  92|    970402.55|\n",
      "|  91|   1137372.73|\n",
      "|  90|   1062654.16|\n",
      "|  89|   1107839.15|\n",
      "|  88|   1031719.35|\n",
      "|  87|    1072285.2|\n",
      "|  86|   1070444.25|\n",
      "|  85|   1056336.74|\n",
      "|  84|   1117233.13|\n",
      "|  83|   1033965.93|\n",
      "|  82|    1063498.0|\n",
      "|  81|   1053472.79|\n",
      "+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "--- 0.6685879230499268 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# 9. Using the cached data, run the last query above, that calculates \n",
    "# the average price of a home per \"view\" rating, rounded to two decimal places,\n",
    "# having an average home price greater than or equal to $350,000. \n",
    "# Determine the runtime and compare it to the uncached runtime.\n",
    "\n",
    "average_price_view_query = \"\"\"\n",
    "SELECT \n",
    "    view, \n",
    "    ROUND(AVG(price), 2) AS average_price \n",
    "FROM \n",
    "    home_sales \n",
    "WHERE \n",
    "    price >= 350000 \n",
    "GROUP BY \n",
    "    view \n",
    "ORDER BY \n",
    "    view DESC\n",
    "\"\"\"\n",
    "\n",
    "# Start the timer for the cached query\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the query using the cached data and create a DataFrame\n",
    "average_price_view_cached_df = spark.sql(average_price_view_query)\n",
    "\n",
    "# Show the results\n",
    "average_price_view_cached_df.show()\n",
    "\n",
    "# Calculate and print the runtime for the cached query\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|                  id|                date|price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|date_built|\n",
      "+--------------------+--------------------+-----+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|�E\u00004\u000eX\u0016@0-99ab-7a...|                NULL| NULL|    NULL|     NULL|       NULL|    NULL|  NULL|      NULL|NULL|      NULL|\n",
      "|�O\u0014b5e-73\u000e0\u0016\u0010c7dd...|                NULL| NULL|    NULL|     NULL|       NULL|    NULL|  NULL|      NULL|NULL|      NULL|\n",
      "|\u0012�\\t\u0000&)�\\f\u0012\u001c",
      "\u0015\u0002m�\u0010...|                NULL| NULL|    NULL|     NULL|       NULL|    NULL|  NULL|      NULL|NULL|      NULL|\n",
      "|\u0000\u0000\u001a�\\b\u0010TP9cc\u000e�!�1...|                NULL| NULL|    NULL|     NULL|       NULL|    NULL|  NULL|      NULL|NULL|      NULL|\n",
      "|L DItiAJ\u0000\u000e!v\u00008Ai\u0004...|                NULL| NULL|    NULL|     NULL|       NULL|    NULL|  NULL|      NULL|NULL|      NULL|\n",
      "|\u0004P\\\u000e;\u0014�H\u0010(c30d6f6...|�\u001f\u001c",
      ":\u0018T\u00141146\u0012A\u0011L\\t...| NULL|    NULL|     NULL|       NULL|    NULL|  NULL|      NULL|NULL|      NULL|\n",
      "|\u0000E\u000e�\u0016\u0000'a\u0018�7\u000e\\b \u00140...|8\u000eL856-8b63-\u0001V�\u0010\u0014...| NULL|    NULL|     NULL|       NULL|    NULL|  NULL|      NULL|NULL|      NULL|\n",
      "|                  a�|$\u000e\u000fF\u0014\u0014#\u001092b\u0001\u000e�\u000f\u0010<...| NULL|    NULL|     NULL|       NULL|    NULL|  NULL|      NULL|NULL|      NULL|\n",
      "| @\u0018\u0016_\\b\u0018\u0014\u00149\u000eR\u0015\u0018\u0012n...|\u0014$\u0018\u0018\u0012\u0010078c\u000e\u0001\u0016�qTF...| NULL|    NULL|     NULL|       NULL|    NULL|  NULL|      NULL|NULL|      NULL|\n",
      "|�a\u0016\u0014\u0014\u0014de-7\u00160I\\7\\b...|                NULL| NULL|    NULL|     NULL|       NULL|    NULL|  NULL|      NULL|NULL|      NULL|\n",
      "|�X\\b47d-\u000eת\u00181e8f\u000e\u0015...|                NULL| NULL|    NULL|     NULL|       NULL|    NULL|  NULL|      NULL|NULL|      NULL|\n",
      "|                 \u0015��|                NULL| NULL|    NULL|     NULL|       NULL|    NULL|  NULL|      NULL|NULL|      NULL|\n",
      "|t2\\b\\f\u0016\u0012Pl O\u0000c\u000e(d...|                NULL| NULL|    NULL|     NULL|       NULL|    NULL|  NULL|      NULL|NULL|      NULL|\n",
      "|                 \u0000Y\u0001|                NULL| NULL|    NULL|     NULL|       NULL|    NULL|  NULL|      NULL|NULL|      NULL|\n",
      "|\u0010-\\f\\t\u0004\\b!\\\u0000#\u000e�E�...|                NULL| NULL|    NULL|     NULL|       NULL|    NULL|  NULL|      NULL|NULL|      NULL|\n",
      "|\u0001\u0000\u0000G\u0010\u0015\\b\u0003\u0000L\u000e.\\t\u0001C...|D\u001c",
      "\u0018\u0012\u0000\u0004%\u001e",
      "\u0005s\u0001oA�\u0012�@...| NULL|    NULL|     NULL|       NULL|    NULL|  NULL|      NULL|NULL|      NULL|\n",
      "|8\u0000\u0004\u0002\u0000\u0005d\u0001\u0019\u0004\u0015\u0018\u0001M\u0005\u0012\u000e...|                NULL| NULL|    NULL|     NULL|       NULL|    NULL|  NULL|      NULL|NULL|      NULL|\n",
      "|�)\u0000\u0000\u0014\u0015\u0014 \\t8\u0000\u0000;7\u0000\u0000...|�)&\u0014\u001b\u0004\\f\u001f\u001e",
      "{\u000eV\u0014\u0004\u0000>...| NULL|    NULL|     NULL|       NULL|    NULL|  NULL|      NULL|NULL|      NULL|\n",
      "|                'n�\u0000|                NULL| NULL|    NULL|     NULL|       NULL|    NULL|  NULL|      NULL|NULL|      NULL|\n",
      "|�dabb95edeH\u001027d85...|                NULL| NULL|    NULL|     NULL|       NULL|    NULL|  NULL|      NULL|NULL|      NULL|\n",
      "+--------------------+--------------------+-----+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11. Read the formatted parquet data.\n",
    "# Define the output path for the partitioned Parquet data\n",
    "output_path = \"/Users/latifahjones/Desktop/Home_Sales/Resources/home_sales_partitioned\"\n",
    "\n",
    "# Read the formatted Parquet data back into a DataFrame\n",
    "df_parquet = spark.read.parquet(output_path)\n",
    "\n",
    "# Show the DataFrame to verify the data\n",
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o213.parquet.\n: java.lang.AssertionError: assertion failed: Conflicting directory structures detected. Suspicious paths:\b\n\tfile:/users/latifahjones/desktop/home_sales/resources/home_sales_partitioned\n\tfile:/users/latifahjones/desktop/home_sales/resources/home_sales_revised.csv\n\tfile:/users/latifahjones/desktop/home_sales/resources/parquet_home_sales\n\nIf provided paths are partition directories, please set \"basePath\" in the options of the data source to specify the root directory of the table. If there are multiple root directories, please load them separately and then union them.\n\tat scala.Predef$.assert(Predef.scala:223)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:178)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:110)\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.inferPartitioning(PartitioningAwareFileIndex.scala:201)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.partitionSpec(InMemoryFileIndex.scala:75)\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.partitionSchema(PartitioningAwareFileIndex.scala:51)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:167)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/latifahjones/Desktop/Home_Sales/Resources/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df_parquet \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(output_path)\n\u001b[1;32m      3\u001b[0m df_parquet\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/sql/readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    542\u001b[0m )\n\u001b[0;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mparquet(_to_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc, paths)))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o213.parquet.\n: java.lang.AssertionError: assertion failed: Conflicting directory structures detected. Suspicious paths:\b\n\tfile:/users/latifahjones/desktop/home_sales/resources/home_sales_partitioned\n\tfile:/users/latifahjones/desktop/home_sales/resources/home_sales_revised.csv\n\tfile:/users/latifahjones/desktop/home_sales/resources/parquet_home_sales\n\nIf provided paths are partition directories, please set \"basePath\" in the options of the data source to specify the root directory of the table. If there are multiple root directories, please load them separately and then union them.\n\tat scala.Predef$.assert(Predef.scala:223)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:178)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:110)\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.inferPartitioning(PartitioningAwareFileIndex.scala:201)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.partitionSpec(InMemoryFileIndex.scala:75)\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.partitionSchema(PartitioningAwareFileIndex.scala:51)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:167)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "output_path = \"/Users/latifahjones/Desktop/Home_Sales/Resources/\"\n",
    "df_parquet = spark.read.parquet(output_path)\n",
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "J6MJkHfvVcvh"
   },
   "outputs": [],
   "source": [
    "# 12. Create a temporary table for the parquet data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_Vhb52rU1Sn",
    "outputId": "a0b8d0c4-55ed-4c6c-bfd8-4c8c5334838e"
   },
   "outputs": [],
   "source": [
    "# 13. Using the parquet DataFrame, run the last query above, that calculates \n",
    "# the average price of a home per \"view\" rating, rounded to two decimal places,\n",
    "# having an average home price greater than or equal to $350,000. \n",
    "# Determine the runtime and compare it to the cached runtime.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hjjYzQGjtbq8",
    "outputId": "830549fd-bb41-451b-9183-5ebf6e3e470b"
   },
   "outputs": [],
   "source": [
    "# 14. Uncache the home_sales temporary table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sy9NBvO7tlmm",
    "outputId": "be73e0e3-5e85-4794-aad9-025fb6fa84a7"
   },
   "outputs": [],
   "source": [
    "# 15. Check if the home_sales is no longer cached\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Home_Sales_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
